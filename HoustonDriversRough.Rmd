---
title: "Houston Drivers Rough"
author: "Will Curkan"
date: "2023-04-15"
output:
  pdf_document: default
  html_document: default
subtitle: A Bayesian Analysis of Moving Violations in Houston
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

## Alternate Introduction 

Southern Texas's "greater Houston area" is a 665 $mi^2$ section of land surrounded and surrounding many municipalities [Cite]. The population of the metropolitan and its suburbs generates much automobile traffic; getting to cities in and around Houston can take hours during the work-school "rush hours". Due to Houston's size and considerable traffic, people feel rushed, break the speed limit laws, and get pulled over. Conveniently, Houston police departments provide their traffic-stop data to the Stanford Policing Project that open-sources the data via their website [Cite].

MAYBE PUT INFO ABOUT DATASET HERE TO FILL FIRST PAGE.

## Introduction


Houston is an enormous metropolitan completely surrounded by suburban areas, denoted "the greater Houston area". The population and the automobile traffic it generates, getting from a suburban area on the south side to one on the north side can consume, more-or-less, an hour and a half of time. With this said, there are lots of cars which means lots of people rushing to get from one municipality or suburb to another, or to go in and out of Houston proper. The want to get from point A to point B in the 665 square mile city causes a subconscious anxiety and desire to rush, which causes police to give many speeding violations. Conveniently, police frequently record the speed of the offender and the posted speed limit when writing a citation, and this data is available via the Stanford Open Policing Project, which the Houston police department participates with in open-sourcing their data.

### Problem

Size and roads cause tons of traffic, accidents. Size is so big it takes forever to get places.

Houston's expanse causes tiring, daily commutes that implore breaking speed limit laws. The posted speed limits in Houston are fair, but only because the roads are tiny [[in hugely populated, high-traffic areas, so there is little room for driving error around the city.]]

Houston's Highway Transit System (HTS) infrastructure is flawed. 

### Purpose

The purpose is to compare the results of a Frequentist analysis and a Bayesian analysis on people stopped by a police officer to hypothesize solutions to traffic problems. The metrics analyzed are: 

QUESTION 1:

- the ratio of actual speed versus posted speed limit, the time between traffic stops, and the number of stops per day.

Actually, I got pulled over once, back in the day, and I recall the "Average speed" in a 60mph area was 75. Given this small amount of data, I'll assume that +25% speed is the $\mu$ with a deviation of something...

QUESTION 2:

- Is there a difference among people driving different national brands of cars?

QUESTION 3:



## Results and Discussion

```{r LIBRARIES, message = F, warning = F}
library(dplyr, quietly = T)
library(ggplot2)
library(rjags)
source("DBDA2E-utilities.R")
```

```{r DATA}
data <- read.csv('tx_houston_2023_01_26.csv')
```

We're intially interested in the distribution of the speed ratio. Let's check out the histogram.

```{r COLNAMES SPARE}
colnames(data)
```


```{r INITIAL RATIO HIST}
# ratio of raw speed to posted speed limit
raw_posted_ratio <- data$speed / data$posted_speed

# Remove the speed ratios that became NA
# due to an undocumented speed 
# (could be many reasons like it not 
# being a speeding violation, or officer error)
raw_posted_ratio <- raw_posted_ratio[!is.na(raw_posted_ratio)]

# Dont need to input a dataframe: NULL
ggplot(NULL, aes(x=raw_posted_ratio)) +
  geom_histogram(bins = 250) +
  geom_vline(aes(xintercept=mean(raw_posted_ratio)),
            color="blue", linetype="dashed", linewidth=1) +
  labs(title = 'Ratio of Raw Speed vs. Posted Speed Limit', x = 'Ratio')
```

```{r INITIAL RATIO SUMMARY}
summary(data$speed[!is.na(data$speed)])
```


There's a huge problem with this distribution: some values are over double, triple, and even 15 times the speed limit. Let's think of this logically. If someone is travelling even five times the speed limit of 20MPH, they're going 100MPH. That's fine, but a road with a 20MPH limit is likely small, so going 100mph almost seems unreasonable. Further, if someone is caught travelling over 10 times the speed limit at 20 or 30MPH, they'd be going 200 and 300MPH, or someone going 60MPH would be travelling at 600MPH. This isn't realistic.

We will consider only vehicles meeting two conditions:
- found to be speeding: their logged speed exceeds the posted speed limit
  - `speed` > `posted_speed`
- Speed is less than 155MPH: the governed speed
  - `speed` $\leq$ 155

`speed` > `posted_speed`

-----The dataset also considers people travelling under the speed limit, which means there were probably pulled over for something other than speeding. These observations are removed as well.------

```{r FILTERED DATASET}
# 155 is the governed speed
# must be speeding; speed > posted_speed
data = data %>%
  filter(speed <= 155, speed > posted_speed)

# hist(data$speed)
# 
# nrow(data)

# qqnorm(data$speed)
# qqline(data$speed)
# 
# hist(data$speed)
# 
# sub1 <- data %>%
#   filter(speed <= 60)
# 
# sub2 <- data %>%
#   filter(speed > 60)
# 
# qqnorm(sub1$speed)
# qqline(sub1$speed)
# 
# qqnorm(sub2$speed)
# qqline(sub2$speed)
# 
# hist(sub1$speed, breaks = 50)
# hist(sub2$speed, breaks = 50)
```

### EDA

```{r FILTERED RATIO AND PLOT}
 ############ REDO THE RAW_POSTED_RATIO #############

# ratio of raw speed to posted speed limit
raw_posted_ratio <- data$speed / data$posted_speed


raw_posted_ratio <- raw_posted_ratio[ raw_posted_ratio > 1 & raw_posted_ratio < 2.5]

length(raw_posted_ratio)

# Remove the speed ratios that became NA
# due to an undocumented speed 
# (could be many reasons like it not 
# being a speeding violation, or officer error)
raw_posted_ratio <- raw_posted_ratio[!is.na(raw_posted_ratio)]

# Dont need to input a dataframe: NULL
ggplot(NULL, aes(x=raw_posted_ratio)) +
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept=mean(raw_posted_ratio)),
            color="blue", linetype="dashed", linewidth=1)

summary(raw_posted_ratio)
sd(raw_posted_ratio)
```


```{r SAMPLE}
# Let's just use a sample of 50 - 200 for the project
# because it doesnt make much sense otherwise
set.seed(42)
x <- sample(raw_posted_ratio, 30)

xbar <- mean(x)
s <- sd(x)


hist(x)
```


So would we trust the sample mean? Obviously there is too much uncertainty, especially with our size n sample.

```{r spare code}
# 611k observations in this range!
# There's 365 less than 1
# There's only 133 ratios over 2.5x
# raw_posted_ratio <- 
#   raw_posted_ratio[(raw_posted_ratio > 1.0) & (raw_posted_ratio < 2.5)]
# 
# ggplot(NULL, aes(x=raw_posted_ratio)) +
#   geom_histogram(bins = 50) +
#   geom_vline(aes(xintercept=mean(raw_posted_ratio)),
#             color="blue", linetype="dashed", linewidth=1)
```

boot mean.

Use lab 8, 9 for reference from stat3333

```{r BOOTSTRAP MEAN}
# set.seed(42)
n <- length(x)

N <- 10^5

boot_means <- numeric(N)
boot_sds <- numeric(N)

for (i in 1:N)
{
  boot_sample <- sample(x, n, replace = T)
  boot_means[i] <- mean(boot_sample)
  boot_sds[i] <- sd(boot_sample)
}

# hist(boot_means)


######################### SKEWED ##############
# qqnorm(boot_means)
# qqline(boot_means)

mean(boot_means)
# sd(boot_means)

mean(boot_sds)
```

### Prior Assumptions

For simplicity, I assume the ratio of speeds is normally distributed.

$\theta$ is the the true mean ratio of speed to speed limit.

We will consider three priors:

1. A noninformative prior assumption on $\theta$

2. $\theta$ based on the sample statistics

3. $\theta$ based on the bootstrap statistics


```{r prior distribution on sample statistics}
set.seed(42)
#######################################################################
############# PRIOR DISTRIBUTION ON SAMPLE STATISTICS #################
#######################################################################
rate <- 1000
shape <- round(s * rate)

Nrep <- 10^5
# Sample mean `xbar` and sample sd `s` from SAMPLE block
mu_sim_prior = rnorm(Nrep, xbar, s)
tau_sim_prior = rgamma(Nrep, shape = shape, rate = rate)
# sigma_sim_prior = 1 / sqrt(tau_sim_prior)
# sim_prior = data.frame(mu_sim_prior, tau_sim_prior, sigma_sim_prior)
ggplot(NULL, aes(mu_sim_prior, tau_sim_prior)) +
  geom_point(color = "skyblue", alpha = 0.4) +
  geom_density_2d(color = "orange", linewidth = 1)
```

```{r prior distribution on bootstrapped statistics}
set.seed(42)
#######################################################################
########## PRIOR DISTRIBUTION ON BOOTSTRAPPED STATISTICS ##############
#######################################################################
rate <- 1000
shape <- round(mean(boot_sds) * rate)

mu_sim_prior = rnorm(Nrep, mean(boot_means), mean(boot_sds))
tau_sim_prior =rgamma(Nrep, shape = shape, rate = rate)
# sigma_sim_prior = 1 / sqrt(tau_sim_prior)
# sim_prior = data.frame(mu_sim_prior, tau_sim_prior, sigma_sim_prior)
ggplot(NULL, aes(mu_sim_prior, tau_sim_prior)) +
  geom_point(color = "skyblue", alpha = 0.4) +
  geom_density_2d(color = "orange", linewidth = 1)
```



1st run:

- A run with my own prior
- A run with noninformative (let data talk)
- A run with sample statistics
- A run with bootstrapped statistics
- A run with hyperpriors

```{r JAGs on Sample Stats}
set.seed(42)
##############################################################################
################### ASSUMPTIONS BASED ON SAMPLE STATISTICS ###################
##############################################################################

n <- length(x)

model_string <- "model{

  # Likelihood
  for (i in 1:n){
    x[i] ~ dnorm(mu, 1 / sigma ^ 2)
  }
  #sigma <- .05
  

  # Prior
  mu ~ dnorm(mu0, 1 / tau0 ^ 2)
  # mu <- 1.393571
  mu0 <-  1.393571
  #mu0 <- 1.33  
  tau0 <- 1 / 0.04827723 # Actually the mean's SD
  sigma <- 0.2696776

}"

# Compile the model
dataList = list(x=x, n=n)
model <- jags.model(textConnection(model_string),
                    data=dataList,
                    n.chains=5, quiet = T) 

update(model, 2000, progress.bar="none")

posterior_sample <- coda.samples(model, 
                                 variable.names=c("mu", 'sigma'),
                                 n.iter=100000,
                                 progress.bar="none")
# Summarize and check diagnostics
summary(posterior_sample)


## --------------------------------
plot(posterior_sample)

#######################################################
#################### DIAGNOSTICS ######################
#######################################################

diagMCMC(posterior_sample,saveName = "diagMCMCeg1",
saveType = "jpg")

#######################################################
########### POSTERIOR PREDICTIVE DISTRIBUTION #########
#######################################################

# theta_sim = as.matrix(posterior_sample)
# x_sim = rnorm(nrow(theta_sim), theta_sim[, "mu"], theta_sim[, "sigma"])
# quantile(x_sim, c(0.025, 0.975))
# hist(x_sim, freq = FALSE, xlab = "Ratio of raw speed to posted limit",
#      main = "Posterior preditive distribution")
# lines(density(x_sim))
# abline(v = quantile(x_sim, c(0.025, 0.975)), col = "orange")

theta_sim = as.matrix(posterior_sample)
x_sim = rnorm(nrow(theta_sim), theta_sim[, "mu"], 0.2696776)#, theta_sim[, "sigma"])
quantile(x_sim, c(0.025, 0.975))
hist(x_sim, freq = FALSE, xlab = "Ratio of raw speed to posted limit",
     main = "Posterior preditive distribution")
lines(density(x_sim))
abline(v = quantile(x_sim, c(0.025, 0.975)), col = "orange")
```



```{r JAGs on Bootstrapped stats}

##############################################################################
############## ASSUMPTIONS BASED ON BOOTSTRAPPED STATISTICS ##################
##############################################################################

n <- length(raw_posted_ratio)

model_string <- "model{

  # Likelihood
  for (i in 1:1000){
    x[i] ~ dnorm(mu, 1 / sigma ^ 2)
  }
  # sigma <- .05
  

  # Prior
  mu ~ dnorm(mu0, 1 / tau0 ^ 2)
  mu0 <- 1.2627  
  # .13
  tau0 ~ dgamma(1,100)
  sigma <- 1 / sqrt(tau0)

}"

# Compile the model
dataList = list(x=raw_posted_ratio, n=n)
model <- jags.model(textConnection(model_string),
                    data=dataList,
                    n.chains=5, quiet = T) 

update(model, 2000, progress.bar="none")

posterior_sample <- coda.samples(model, 
                                 variable.names=c("mu", 'sigma'),
                                 n.iter=N,
                                 progress.bar="none")
# Summarize and check diagnostics
summary(posterior_sample)


## --------------------------------
plot(posterior_sample)

#######################################################
#################### DIAGNOSTICS ######################
#######################################################

# diagMCMC(posterior_sample,saveName = "diagMCMCeg",
# saveType = "jpg")
theta_sim = as.matrix(posterior_sample)
x_sim = rnorm(nrow(theta_sim), theta_sim[, "mu"], theta_sim[, "sigma"])
quantile(x_sim, c(0.025, 0.975))
hist(x_sim, freq = FALSE, xlab = "Ratio of raw speed to posted limit",
     main = "Posterior preditive distribution")
lines(density(x_sim))
abline(v = quantile(x_sim, c(0.025, 0.975)), col = "orange")
```


