---
title: "Houston Drivers Rough"
author: "Will Curkan"
date: "2023-04-15"
output:
  pdf_document: default
  html_document: default
subtitle: A Bayesian Analysis of Moving Violations in Houston
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

## Alternate Introduction 

Southern Texas's "greater Houston area" is a 665 $mi^2$ section of land surrounded and surrounding many municipalities [Cite]. The population of the metropolitan and its suburbs generates much automobile traffic; getting to cities in and around Houston can take hours during the work-school "rush hours". Due to Houston's size and considerable traffic, people feel rushed, break the speed limit laws, and get pulled over. Conveniently, Houston police departments provide their traffic-stop data to the Stanford Policing Project that open-sources the data via their website [Cite].

MAYBE PUT INFO ABOUT DATASET HERE TO FILL FIRST PAGE.

## Introduction


Houston is an enormous metropolitan completely surrounded by suburban areas, denoted "the greater Houston area". The population and the automobile traffic it generates, getting from a suburban area on the south side to one on the north side can consume, more-or-less, an hour and a half of time. With this said, there are lots of cars which means lots of people rushing to get from one municipality or suburb to another, or to go in and out of Houston proper. The want to get from point A to point B in the 665 square mile city causes a subconscious anxiety and desire to rush, which causes police to give many speeding violations. Conveniently, police frequently record the speed of the offender and the posted speed limit when writing a citation, and this data is available via the Stanford Open Policing Project, which the Houston police department participates with in open-sourcing their data.

### Problem

Size and roads cause tons of traffic, accidents. Size is so big it takes forever to get places.

Houston's expanse causes tiring, daily commutes that implore breaking speed limit laws. The posted speed limits in Houston are fair, but only because the roads are tiny [[in hugely populated, high-traffic areas, so there is little room for driving error around the city.]]

Houston's Highway Transit System (HTS) infrastructure is flawed. 

### Purpose

The purpose is to compare the results of a Frequentist analysis and a Bayesian analysis on people stopped by a police officer to hypothesize solutions to traffic problems. The metrics analyzed are: 

- the ratio of actual speed versus posted speed limit, the time between traffic stops, and the number of stops per day.

Actually, I got pulled over once, back in the day, and I recall the "Average speed" in a 60mph area was 75. Given this small amount of data, I'll assume that +25% speed is the $\mu$ with a deviation of something...

## Results and Discussion

```{r libraries}
library(dplyr, quietly = T)
library(ggplot2)
library(rjags)
```

```{r data}
data <- read.csv('tx_houston_2023_01_26.csv')
```

### EDA

```{r rawSpeed_postedSpeed_ratio}
# ratio of raw speed to posted speed limit
raw_posted_ratio <- data$speed / data$posted_speed

# Remove the speed ratios that became NA
# due to an undocumented speed 
# (could be many reasons like it not 
# being a speeding violation, or officer error)
raw_posted_ratio <- raw_posted_ratio[!is.na(raw_posted_ratio)]

# Dont need to input a dataframe: NULL
ggplot(NULL, aes(x=raw_posted_ratio)) +
  geom_histogram(bins = 250) +
  geom_vline(aes(xintercept=mean(raw_posted_ratio)),
            color="blue", linetype="dashed", linewidth=1)
```

There's a huge problem with this distribution: some values are over double and triple the speed limit. Let's think of this logically. If someone is travelling even five times the speed limit of 20MPH, they're going 100MPH. That's fine, but a road with a 20MPH limit is likely small, so going 100mph almost seems unreasonable. On an even bigger scale, if someone got caught travelling over 10 times the speed limit at 20 or 30MPH, they'd be going 200 and 300MPH, or someone going 60MPH would be travelling at 600MPH. This isn't realistic.

The dataset also considers people travelling under the speed limit, which means there were probably pulled over for something other than speeding.

We will only consider persons travelling one to two-and-a-half times the speed limit.

```{r}
# 611k observations in this range!
# There's 365 less than 1
# There's only 133 ratios over 2.5x
raw_posted_ratio <- 
  raw_posted_ratio[(raw_posted_ratio > 1.0) & (raw_posted_ratio < 2.5)]

ggplot(NULL, aes(x=log(raw_posted_ratio))) +
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept=mean(log(raw_posted_ratio))),
            color="blue", linetype="dashed", linewidth=1)
```

boot mean.

Use lab 8, 9 for reference from stat3333

```{r}
n <- length(raw_posted_ratio) * .5

N <- 10^4

boot_means <- numeric(N)

for (i in 1:N)
{
  boot_means[i] <- mean(sample(raw_posted_ratio, n, replace = T))
}

hist(boot_means)

qqnorm(boot_means)
qqline(boot_means)

mean(boot_means)
sd(boot_means)
```

```{r}

n <- length(raw_posted_ratio)

model_string <- "model{

  # Likelihood
  for (i in 1:n){
    x[i] ~ dnorm(theta, 1 / sigma ^ 2)
  }
  sigma <- 600
  

  # Prior
  theta ~ dnorm(mu0, 1 / tau0 ^ 2)
  mu0 <- 3000  
  tau0 <- 150

}"

# Compile the model
dataList = list(x=raw_posted_ratio, n=n)
model <- jags.model(textConnection(model_string),
                    data=dataList,
                    n.chains=5) 

update(model, 1000, progress.bar="none")

posterior_sample <- coda.samples(model, 
                                 variable.names=c("theta"),
                                 n.iter=Nrep,
                                 progress.bar="none")
# Summarize and check diagnostics
summary(posterior_sample)


## --------------------------------
plot(posterior_sample)
```

